#!/bin/bash

# we ge an action + cube name or action + json
action=$1
shift

# if we have json, there will be { in the next input, otherwise, it
# is a directory
echo "$@" | grep -q {
if [ $? -eq 0 ]; then
    # json
    cubedir=$(pwd)
    cubename=$(basename $cubedir)

    # yank what we need out of the json
    pid=$(echo "$@" | jq .pid)
    veth_name=$(cat ${cubedir}/.cube.veth)
else
    # directory
    cubedir=$1
    cubename=$(basename $cubedir)
    pid=$2
    veth_name=$(cat ${cubedir}/.cube.veth)
fi

cd $cubedir

if [ -z "${pid}" ]; then
    if [ -f ".cube.pid" ]; then
	pid=$(cat .cube.pid)
    fi
fi

get_vrf_container() {
    # our default answer
    vrf_container="cube-vrf"
    # TODO: brute force. Could be an etcd lookup
    for c in ls /opt/container/*; do
	if [ -e "${c}/cube.attributes" ]; then
	    cat ${c}/cube.attributes | grep -q -w vrf
	    if [ $? -eq 0 ]; then
		vrf_container=$(basename ${c})
	    fi
	fi
    done

    echo "${vrf_container}"
}

if [ "${action}" = "netprime" ]; then
    netdevs=$(cat .cube.device.network)
    vrf=$(get_vrf_container)
    devmgr="essential"
    if [ -f cube.device.mgr ]; then
	devmgr=$(cat cube.device.mgr)
    fi

    # Expand any network+ args
    netdevsexp=""
    nds=`nsenter -t ${pid} -n ip -o link  |awk -F: '{print $2}'`
    for n in ${netdevs}; do
        if [ "${n}" = "${n/+/}" ] ; then
            netdevsexp="$n $netdevsexp"
        else
            device=$(echo ${n} | cut -f1 -d+)
            for nd in $nds; do
                if [ "$nd" != "${nd#$device}" ] ; then
                    netdevsexp="$nd:$nd $netdevsexp"
                fi
            done
        fi
    done

    # In self managed mode with a netprime, the cube-vrf will get
    # its injected file changed to a symlink, which is monitoried by
    # inotify in the vrf which can allow the netprime's file system
    # to come and go at will.

    if [ "${devmgr}" = self ] ; then
	rm -f /opt/container/${vrf}/rootfs/etc/vrf-resolv.conf
	ln -s /opt/container/${cubename}/rootfs/etc/resolv.conf /opt/container/${vrf}/rootfs/etc/vrf-resolv.conf
    fi

    for n in ${netdevsexp}; do

	# When the network is set to none, all bets are off, and the
	# and the end container must manage all settings for the VRF
	# so that DNS works correctly for all containers on the system
	if [ "${devmgr}" = "none" ] ; then
	    break;
	fi

	echo ${n} | grep -q veth
	if [ $? -ne 0 ]; then
	    # we have a real device
	    device=$(echo ${n} | cut -f1 -d:)

	    # forwarding (for other containers to the external network)
	    nsenter -t ${pid} -n --preserve-credentials iptables -t nat -A POSTROUTING -o ${device} -j MASQUERADE
	    nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo 1 > /proc/sys/net/ipv4/ip_forward'
	    # In network = "self" mode, the monitoring of the DNS
	    # configuration is already setup, so the rest is skipped.
	    if [ "${devmgr}" = self ] ; then
		continue;
	    fi

	    # Up the interface via dhclient, since not all netprime
	    # containers are capable of doing this themselves

	    ## TODO: This might be possible as an optional step. Perhaps just checking to see
	    ##       if the device/hw is self managed ? or an attribute on the container ?
	    ##       Can we just tag any type of h/w as managed ? versus it all ?
	    ##       or is this network specific and we just set this at "none" or "self" and
	    ##       leave it be .. it is very likely better as a network type.
	    ##
	    ##       The problem is. if we leave this self managed, we cannot set the DNS information
	    ##       on the VRF and that's an issue.
	    # the ip must be in the format of address/mask, i.e.: 192.168.42.101/24
	    ip=$(cat cube.network.external.ip)
	    if [ -n "${ip}" ]; then
		# the first entry is the primary, any others are aliases
		count=0
		for i in ${ip}; do
		    if [ ${count} -eq 0 ]; then
			nsenter -t ${pid} -n --preserve-credentials ip addr add $i dev ${device}
			primary_ip=$(echo $i | cut -d/ -f1)
		    else
			nsenter -t ${pid} -n --preserve-credentials ip addr add $i dev ${device}:${count}
		    fi
		    let count=${count}+1
		done

		# todo: should specify a gateway
		nsenter -t ${pid} -n --preserve-credentials route add default ${device}

		nameserver="192.168.42.1"
		if [ -f "cube.network.external.nameserver" ]; then
		    nameserver=$(cat cube.network.external.nameserver)
		fi
		echo "${nameserver}" > /opt/container/${cubename}/resolv.conf

		# On a system with /var/run the /etc/resolv.conf will be a volatile
		#nsenter -t ${pid} -m sh -c "if [ -e /var/run ] ; then echo nameserver ${nameserver} > /var/run/resolv.conf; ln -sf /var/run/resolv.conf /etc/resolv.conf; else rm -f /etc/resolv.conf; echo nameserver ${nameserver} > /etc/resolv.conf; fi"
	    else
		command="dhclient -lf /opt/container/${cubename}/rootfs/var/lib/dhcp/dhclient.leases -sf /usr/sbin/dhclient-script.container -e CONTAINER=${cubename} --no-pid ${device} >> /dev/null 2>&1"
		# Run the command in the network prime's pid space such that when
		# the container is killed so is the command, should it be a daemon
		eval nsenter -p -t ${pid} -n -- ${command}
	    fi

	    if [ -e "/opt/container/${cubename}/resolv.conf" ]; then

		# Write out DNS forwarding information into the vrf
		echo "# servers in this file are managed by the cube-netconfig hook. Any changes will be overwritten" > /opt/container/${vrf}/rootfs/etc/vrf-resolv.conf
		cat /opt/container/${cubename}/resolv.conf >> /opt/container/${vrf}/rootfs/etc/vrf-resolv.conf

		# These may need to be refined, but they nat and forward and DNS
		# traffic from the netprime (.1) to the VRF (currently "the vrf" @ .4)
		# TODO: the IPs could be variables

		# check to make sure we aren't both .4 and .1 .. since we don't need any
		# port forwards in that case
		forward_dns_to_vrf=t
		if [ -f "cube.network.ip" ]; then
		    for ip in $(cat cube.network.ip); do
			ip_no_mask=${ip%/*}
			if [ "${ip_no_mask}" == "192.168.42.4" ]; then
			    forward_dns_to_vrf=
			fi
		    done
		fi

		if [ -n "${forward_dns_to_vrf}" ]; then
		    for ip in 192.168.42.1; do
			nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -A PREROUTING  -d ${ip} -p tcp -m tcp --dport 53 -j DNAT --to-destination 192.168.42.4:53
			nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -A PREROUTING  -d ${ip} -p udp -m udp --dport 53 -j DNAT --to-destination 192.168.42.4:53
		    done
		    # we need localhost, or the netprime won't have working DNS itself
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -A OUTPUT -t nat -p tcp --dport 53 -j DNAT --to 192.168.42.4:53
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -A OUTPUT -t nat -p udp --dport 53 -j DNAT --to 192.168.42.4:53

		    # TODO. this shouldn't be run over and over and over ...
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -I POSTROUTING -d 192.168.42.0/24 -j MASQUERADE
		fi
	    fi
	
	    # We need to track the pids of dhclient so it can be killed later if the
	    # container dies. Since dhclient isn't in that pid namespace, it will live
	    # on and keep the network device from returning to the root namespace
	    rm -f /opt/container/${cubename}/.netns.pids
	    for i in $(pidof dhclient); do
		ns=$(ip netns identify $i)
		if [ "${ns}" == "${cubename}" ]; then
		    echo $i >> /opt/container/${cubename}/.netns.pids
		fi
	    done
	fi
    done
fi

if [ "${action}" = "vrf" ]; then
    echo "[INFO]: VRF setup"

    nsenter -t ${pid} -m grep -q ${veth_name} /etc/dnsmasq.conf
    if [ $? -ne 0 ]; then
	# dnsmasq setup
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c "echo \"interface=${veth_name}\" >> /etc/dnsmasq.conf"
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo "dhcp-range=192.168.42.100,192.168.42.200,2h" >> /etc/dnsmasq.conf'
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo "dhcp-option=option:router,192.168.42.1" >> /etc/dnsmasq.conf'

	# dnsmasq is in the vrf, and we are the vrf, so there's no need to
	# restart dnsmasq in this hook.
    fi

    # create the local lan configuration. When the oci-network hooks run, they'll
    # populate the vrf's host file so we can have resolution on <container>.cube.lan
    nsenter -t ${pid} -m echo "local=/cube.lan/" >> /etc/dnsmasq.conf
    nsenter -t ${pid} -m echo "expand-hosts" >> /etc/dnsmasq.conf
    nsenter -t ${pid} -m echo "domain=cube.lan" >> /etc/dnsmasq.conf

    ## TODO: in a self managed netprime configuration the VRF needs to go get the
    ##       dns information. If the netprime is doing it above (which it always
    ##       does now, we are covered.
fi
