#/bin/bash

# we ge an action + cube name or action + json
action=$1
shift

# if we have json, there will be { in the next input, otherwise, it
# is a directory
echo "$@" | grep -q {
if [ $? -eq 0 ]; then
    # json
    cubedir=$(pwd)
    cubename=$(basename $cubedir)

    # yank what we need out of the json
    pid=$(echo "$@" | jq .pid)
    veth_name=$(cat ${cubedir}/.cube.veth)
else
    # directory
    cubedir=$1
    cubename=$(basename $cubedir)
    pid=$2
    veth_name=$(cat ${cubedir}/.cube.veth)
fi

cd $cubedir

if [ -z "${pid}" ]; then
    if [ -f ".cube.pid" ]; then
	pid=$(cat .cube.pid)
    fi
fi

if [ "${action}" = "netprime" ]; then
    netdevs=$(cat .cube.device.network)
    for n in ${netdevs}; do
	echo ${n} | grep -q veth
	if [ $? -ne 0 ]; then
	    # we have a real device
	    device=$(echo ${n} | cut -f1 -d:)

	    # forwarding (for other containers to the external network)
	    nsenter -t ${pid} -n --preserve-credentials iptables -t nat -A POSTROUTING -o ${device} -j MASQUERADE
	    nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo 1 > /proc/sys/net/ipv4/ip_forward'

	    # Up the interface via dhclient, since not all netprime
	    # containers are capable of doing this themselves

	    ## TODO: This might be possible as an optional step. Perhaps just checking to see
	    ##       if the device/hw is self managed ? or an attribute on the container ?
	    ##       Can we just tag any type of h/w as managed ? versus it all ?
	    ##       or is this network specific and we just set this at "none" or "self" and
	    ##       leave it be .. it is very likely better as a network type.
	    ##
	    ##       The problem is. if we leave this self managed, we cannot set the DNS information
	    ##       on the VRF and that's an issue.
	    command="dhclient -sf /usr/sbin/dhclient-script.container -e CONTAINER=${cubename} --no-pid ${device} >> /dev/null 2>&1"
	    eval nsenter -t ${pid} -n -- ${command}
	    if [ -e "/etc/resolv.conf.${cubename}" ]; then
		# the VRF (currently dom0) routes our DNS traffic, so we need to get
		# this captured config into it's dnsmasq.conf

		# temp. this should just be a lookup, but hard coding to test the new VRF code
		vrf="cube-vrf"
		if [ -e ".cube.vrf.name" ]; then
		    vrf=$(cat .cube.vrf.name)
		fi

		# clean up, before we add our options
		sed '/no-resolv/d' -i /opt/container/${vrf}/rootfs/etc/dnsmasq.conf
		sed '/server=.*/d' -i /opt/container/${vrf}/rootfs/etc/dnsmasq.conf

		sed '/search.*/d' -i /etc/resolv.conf.${cubename}
		sed 's/nameserver /server=/g' -i /etc/resolv.conf.${cubename}

		echo "# servers in this file are managed by the cube-netconfig hook. Any changes will be overwritten" >> /opt/container/${vrf}/rootfs/etc/dnsmasq.conf
		echo "no-resolv" >> /opt/container/${vrf}/rootfs/etc/dnsmasq.conf
		cat /etc/resolv.conf.${cubename} >> /opt/container/${vrf}/rootfs/etc/dnsmasq.conf

		# These may need to be refined, but they nat and forward and DNS
		# traffic from the netprime (.1) to the VRF (currently dom0 @ .3)
		# TODO: the IPs could be variables

		# check to make sure we aren't both .3 and .1 .. since we don't need any
		# port forwards in that case
		forward_dns_to_vrf=t
		# if [ -f "cube.network.ip" ]; then
		#     for ip in $(cat cube.network.ip); do
		# 	ip_no_mask=${ip%/*}
		# 	if [ "${ip_no_mask}" == "192.168.42.3" ]; then
		# 	    forward_dns_to_vrf=
		# 	fi
		#     done
		# fi

		if [ -n "${forward_dns_to_vrf}" ]; then
		    for ip in 192.168.42.1; do
			nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -A PREROUTING  -d ${ip} -p tcp -m tcp --dport 53 -j DNAT --to-destination 192.168.42.4:53
			nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -A PREROUTING  -d ${ip} -p udp -m udp --dport 53 -j DNAT --to-destination 192.168.42.4:53
		    done
		    # we need localhost, or the netprime won't have working DNS itself
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -A OUTPUT -t nat -p tcp --dport 53 -j DNAT --to 192.168.42.4:53
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -A OUTPUT -t nat -p udp --dport 53 -j DNAT --to 192.168.42.4:53

		    # TODO. this shouldn't be run over and over and over ...
		    nsenter -t ${pid} -n --preserve-credentials -- iptables -t nat -I POSTROUTING -d 192.168.42.0/24 -j MASQUERADE
		fi

		# Enter dom0 and restart dnsmasq
		dom0_pid=$(cat /opt/container/dom0/.cube.pid)
		nsenter -t ${dom0_pid} -m -n -u -i -p -C --preserve-credentials -- systemctl restart dnsmasq
	    fi
	
	    # We need to track the pids of dhclient so it can be killed later if the
	    # container dies. Since dhclient isn't in that pid namespace, it will live
	    # on and keep the network device from returning to the root namespace
	    rm -f /opt/container/${cubename}/.netns.pids
	    for i in $(pidof dhclient); do
		ns=$(ip netns identify $i)
		if [ "${ns}" == "${cubename}" ]; then
		    echo $i >> /opt/container/${cubename}/.netns.pids
		fi
	    done
	fi
    done
fi

if [ "${action}" = "vrf" ]; then
    echo "[INFO]: VRF setup"

    nsenter -t ${pid} -m grep -q ${veth_name} /etc/dnsmasq.conf
    if [ $? -ne 0 ]; then
	# dnsmasq setup
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c "echo \"interface=${veth_name}\" >> /etc/dnsmasq.conf"
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo "dhcp-range=192.168.42.100,192.168.42.200,2h" >> /etc/dnsmasq.conf'
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials sh -c 'echo "dhcp-option=option:router,192.168.42.1" >> /etc/dnsmasq.conf'
	nsenter -t ${pid} -m -n -u -i -p -C --preserve-credentials systemctl restart dnsmasq
	if [ $? -ne 0 ]; then
	    exit 0
	fi
    fi

    # create the local lan configuration. When the oci-network hooks run, they'll
    # populate the vrf's host file so we can have resolution on <container>.cube.lan
    nsenter -t ${pid} -m echo "local=/cube.lan/" >> /etc/dnsmasq.conf
    nsenter -t ${pid} -m echo "expand-hosts" >> /etc/dnsmasq.conf
    nsenter -t ${pid} -m echo "domain=cube.lan" >> /etc/dnsmasq.conf

    ## TODO: in a self managed netprime configuration the VRF needs to go get the
    ##       dns information. If the netprime is doing it above (which it always
    ##       does now, we are covered.
fi
